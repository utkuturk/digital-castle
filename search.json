[
  {
    "objectID": "posts/mfa/index.html",
    "href": "posts/mfa/index.html",
    "title": "Guidelines to Run Montreal Forced Aligner for a PCIbex Experiment",
    "section": "",
    "text": "We have our data from PCIbex and our zip files from the server. Let’s assume we have unzipped them and converted them from .webm to .wav files. Now, it’s time to align them using MFA. But before that, we need to prepare our files accordingly. Here are the steps we will follow in this document:\n\nLoad the PCIbex results\nFilter out irrelevant sound files\nMove all of our .wav and .TextGrid files to the same directory\nRename our files according to MFA guidelines\nRun MFA\nCreate a dataframe\n\nBefore we start, let me load my favorite packages. The library() function loads the packages we need, assuming they are already installed. If not, use the install.packages() function to install them. While library() does not require quotes, you should use quotes with install.packages(), e.g., install.packages(\"tidyverse\"). If it asks you to select a mirror from a list, choose a location geographically close to you, such as the UK.\n\nlibrary(tidyverse) # I have to have tidyverse\nlibrary(stringr) # to manipulate string\nlibrary(readtextgrid) # to read TextGrid files\nlibrary(dplyr)"
  },
  {
    "objectID": "posts/mfa/index.html#read-the-results",
    "href": "posts/mfa/index.html#read-the-results",
    "title": "Guidelines to Run Montreal Forced Aligner for a PCIbex Experiment",
    "section": "\n2.1 Read the results",
    "text": "2.1 Read the results\nThe main reason we are loading PCIbex results is because sometimes we use the async() function in our PCIbex code. The async() function allows us to send recordings to our server whenever we want without waiting for the end of the experiment. Even though it is extremely helpful in reducing some of the server-PCIbex connection load at the end of the experiment, it also creates some pesky situations. For example, if a participant decides not to complete their experiment, we will still end up with some of their recordings. We do not want participants who were window-shopping, mainly because we are not sure about the quality of their data. Luckily for us, PCIbex only saves the results of participants who complete the entire experiment.\nTo read the PCIbex results, we are going to use the function provided in the PCIbex documentation. Scroll down on that page, and you will see the words “Click for Base R Version.” The function is provided there as well. Moreover, please be careful whenever you are copying and pasting functions from this file, or any file, as sometimes PDF or HTML files can include unwanted elements, like a page number.\n\n# User-defined function to read in PCIbex Farm results files\nread.pcibex &lt;- function(\n    filepath, \n    auto.colnames=TRUE, \n    fun.col=\\(col,cols){cols[cols==col]&lt;-paste(col,\"Ibex\",sep=\".\");return(cols)}\n    ) {\n  n.cols &lt;- max(count.fields(filepath,sep=\",\",quote=NULL),na.rm=TRUE)\n  if (auto.colnames){\n    cols &lt;- c()\n    con &lt;- file(filepath, \"r\")\n    while ( TRUE ) {\n      line &lt;- readLines(con, n = 1, warn=FALSE)\n      if ( length(line) == 0) {\n        break\n      }\n      m &lt;- regmatches(line,regexec(\"^# (\\\\d+)\\\\. (.+)\\\\.$\",line))[[1]]\n      if (length(m) == 3) {\n        index &lt;- as.numeric(m[2])\n        value &lt;- m[3]\n        if (is.function(fun.col)){\n         cols &lt;- fun.col(value,cols)\n        }\n        cols[index] &lt;- value\n        if (index == n.cols){\n          break\n        }\n      }\n    }\n    close(con)\n    return(read.csv(filepath, comment.char=\"#\", header=FALSE, col.names=cols))\n  }\n  else{\n    return(read.csv(filepath, comment.char=\"#\", header=FALSE, col.names=seq(1:n.cols)))\n  }\n}\n\nSo, now what we have to do is load our file. I also want to check my file using the str() function. Please run ?str to see what this function does. For any function that you do not understand, you can run the ? operator to see the help pages and some examples.\n\nibex &lt;- read.pcibex(\"./octo-recall-ibex.csv\")\nstr(ibex)\n\n\n\n'data.frame':   15265 obs. of  23 variables:\n $ Results.reception.time              : int  1716599254 1716599254 1716599254 1716599254 1716599254 1716599254 1716599254 1716599254 1716599254 1716599254 ...\n $ MD5.hash.of.participant.s.IP.address: chr  \"eb9865e3e6ad96f9ff9db3a12b5565ae\" \"eb9865e3e6ad96f9ff9db3a12b5565ae\" \"eb9865e3e6ad96f9ff9db3a12b5565ae\" \"eb9865e3e6ad96f9ff9db3a12b5565ae\" ...\n $ Controller.name                     : chr  \"PennController\" \"PennController\" \"PennController\" \"PennController\" ...\n $ Order.number.of.item                : int  1 1 1 1 1 1 1 1 1 1 ...\n $ Inner.element.number                : int  0 0 0 0 0 0 0 0 0 0 ...\n $ Label                               : chr  \"consent_form\" \"consent_form\" \"consent_form\" \"consent_form\" ...\n $ Latin.Square.Group                  : chr  \"NULL\" \"NULL\" \"NULL\" \"NULL\" ...\n $ PennElementType                     : chr  \"PennController\" \"PennController\" \"PennController\" \"Html\" ...\n $ PennElementName                     : chr  \"0\" \"0\" \"0\" \"consent\" ...\n $ Parameter                           : chr  \"_Trial_\" \"_Header_\" \"_Header_\" \"prolificID\" ...\n $ Value                               : chr  \"Start\" \"Start\" \"End\" \"66481a6d648519a547aebd68\" ...\n $ EventTime                           : chr  \"1716597822554\" \"1716597822554\" \"1716597822554\" \"1716597873803\" ...\n $ w4                                  : chr  \"undefined\" \"undefined\" \"undefined\" \"undefined\" ...\n $ w3                                  : chr  \"undefined\" \"undefined\" \"undefined\" \"undefined\" ...\n $ w2                                  : chr  \"undefined\" \"undefined\" \"undefined\" \"undefined\" ...\n $ w1                                  : chr  \"undefined\" \"undefined\" \"undefined\" \"undefined\" ...\n $ nw                                  : chr  \"undefined\" \"undefined\" \"undefined\" \"undefined\" ...\n $ itemnum                             : chr  \"undefined\" \"undefined\" \"undefined\" \"undefined\" ...\n $ trigger_type                        : chr  \"undefined\" \"undefined\" \"undefined\" \"undefined\" ...\n $ trigger                             : chr  \"undefined\" \"undefined\" \"undefined\" \"undefined\" ...\n $ verb_type                           : chr  \"undefined\" \"undefined\" \"undefined\" \"undefined\" ...\n $ head                                : chr  \"undefined\" \"undefined\" \"undefined\" \"undefined\" ...\n $ Comments                            : chr  \"NULL\" \"NULL\" \"NULL\" \"text input\" ...\n\n\nNow, what I want to do is to get to filenames that are recorded in the PCIbex results. Before doing that, I advise you to go to this documentation and read more about PCIbex under the Basic Concepts header.\n\n\n\n\n\n\nColumn\nInformation\n\n\n\n1\nTime results were received (seconds since Jan 1 1970)\n\n\n2\nMD5 hash identifying subject. This is based on the subject’s IP address and various properties of their browser. Together with the value of the first column, this value should uniquely identify each subject.\n\n\n3\nName of the controller for the entity (e.g. “DashedSentence”)\n\n\n4\nItem number\n\n\n5\nElement number\n\n\n6\nLabel. Label of the newTrial()\n\n\n7\nLatin.Square.Group. The group they are assigned to.\n\n\n8\nPennElementType. Name of the specific element, like “Html”, “MediaRecorder”\n\n\n9\nPennElementName. Name we have given to the specific Penn Elements.\n\n\n10\nParameter. This is about what type of element the script is running and saving as a parameter.\n\n\n11\nValue. Value saved for the parameters in column 10.\n\n\n12\nEventTime. Time that specific Element is screened or any action taken with that Element (seconds since Jan 1 1970)"
  },
  {
    "objectID": "posts/mfa/index.html#filter-the-results",
    "href": "posts/mfa/index.html#filter-the-results",
    "title": "Guidelines to Run Montreal Forced Aligner for a PCIbex Experiment",
    "section": "\n2.2 Filter the results",
    "text": "2.2 Filter the results\nSince we are dealing with media recordings, I will first filter the file using the PennElementType column and will only select rows with “MediaRecorder” values in that column.\n\nibex &lt;- ibex |&gt; filter(PennElementType == \"MediaRecorder\")\nunique(ibex$Value)[1:3]\n\n[1] \"test-recorder.webm\" \"1_prac-1_jztb.webm\" \"2_prac-1_jztb.webm\"\n\n\nAfter checking my Value column where the file names for MediaRecorder are stored, I realize that this will not be enough given that we still have other unwanted elements like test-recorder.webm file or some practice files. There are multiple ways to get rid of these files, and you have to think about how to get rid of them for your own specific dataframe. For my own data, I will filter my data utilizing the labels I provided in my PCIbex code. They are stored in the Labels column. What I want is to only get the MediaRecorders that are within a trial whose label starts with the word trial.\nYou may have coded your data differently; you may have used a different word; you may not even have any practice or test-recorders, so maybe you do not even need this second filtering. Check your dataframe using the View() function. I am also using a function called str_detect(), which detects a regular expression pattern, in this case ^trial, meaning starting with the word trial. Now, when I check my dataframe, I will only see experimental trials and recordings related to those trials. Just to make sure, I am also using the unique() function so that I do not have repetitions. And, I am assigning my filenames to a list called ibex_files. You can see that any random sample with the sample() function will give filenames related to experimental trials.\n\nibex &lt;- ibex |&gt; filter(str_detect(Label, \"^trial\"))\nibex_files &lt;- ibex$Value |&gt; unique()\nsample(ibex_files, 3)\n\n[1] \"dog_Unergative_unrelated_soaw.webm\"  \n[2] \"snail_Unergative_unrelated_tvbh.webm\"\n[3] \"chef_Unergative_related_uopp.webm\""
  },
  {
    "objectID": "posts/mfa/index.html#operators-used-in-this-section",
    "href": "posts/mfa/index.html#operators-used-in-this-section",
    "title": "Guidelines to Run Montreal Forced Aligner for a PCIbex Experiment",
    "section": "\n2.3 Operators used in this section",
    "text": "2.3 Operators used in this section\n\n?\nOpens the help page for any function.\nexample use: ?library()\n\n\n==\nTest for equality. Don’t confuse with a single =, which is an assignment operator (and also always returns TRUE).\nexample use: ``\n\n\n\n|&gt;\n(Forward) pipe: Use the expression on the left as a part of the expression on the right.\n\nRead x |&gt; fn() as ‘use x as the only argument of function fn’.\n\nRead x |&gt; fn(1, 2) as ‘use x as the first argument of function fn’.\n\nRead x |&gt; fn(1, ., 2) as ‘use x as the second argument of function fn’.\n\n\nexample use: ``"
  },
  {
    "objectID": "posts/mfa/index.html#our-task-list",
    "href": "posts/mfa/index.html#our-task-list",
    "title": "Guidelines to Run Montreal Forced Aligner for a PCIbex Experiment",
    "section": "\n2.4 Our Task List",
    "text": "2.4 Our Task List\n\nLoad the PCIbex results\nFilter irrelevant sound files\nMove all of our .wav and .TextGrid files to the same directory\nRename our files according to MFA guidelines\nRun MFA\nCreate a dataframe"
  },
  {
    "objectID": "posts/mfa/index.html#operators-used-in-this-section-1",
    "href": "posts/mfa/index.html#operators-used-in-this-section-1",
    "title": "Guidelines to Run Montreal Forced Aligner for a PCIbex Experiment",
    "section": "\n3.1 Operators used in this section",
    "text": "3.1 Operators used in this section\n\n%in%\nTest for membership\nexample use: ``"
  },
  {
    "objectID": "posts/mfa/index.html#our-task-list-1",
    "href": "posts/mfa/index.html#our-task-list-1",
    "title": "Guidelines to Run Montreal Forced Aligner for a PCIbex Experiment",
    "section": "\n3.2 Our Task List",
    "text": "3.2 Our Task List\n\nLoad the PCIbex results\nFilter irrelevant sound files\nMove all of our .wav and .TextGrid files to the same directory\nRename our files according to MFA guidelines\nRun MFA\nCreate a dataframe"
  },
  {
    "objectID": "posts/mfa/index.html#one-file-example",
    "href": "posts/mfa/index.html#one-file-example",
    "title": "Guidelines to Run Montreal Forced Aligner for a PCIbex Experiment",
    "section": "\n4.1 One-file example",
    "text": "4.1 One-file example\nMy files, after unzipping, converting to .wav, and filtering according to the gold list, look like the following. There are some important things to keep in mind here. First of all, now that everything is in our gold directory, we have to use the gold_dir variable to list our files. Secondly, we again need to use the pattern argument to make sure we only select relevant files. The last thing to be aware of in the next code is that I am using indexing with square brackets to refer to the first elements in the list. I will use this element to first ensure that what I am doing is correct.\n\ngold_dir &lt;- \"~/data/gold\"\nfiles &lt;- list.files(gold_dir, pattern = \"\\\\.wav$|\\\\.TextGrid$\")\nexample_file &lt;- files[1]\nexample_file\n\n[1] \"shota-rep-trial_ballerinaUnacc_sgsg_related_dltc.TextGrid\"\n\n\n\n4.1.1 Get the extension and the name\nNow that we have the name of an example file, we can start by extracting its extension. We will use the function file_ext from the package called tools. Sometimes, we do not want to load an entire package, but we want to access a single function. In those cases, we use the operator ::. Additionally, we will use paste0 to prefix the extension with a dot, so that we can use it later when we rename our files.\n\nextension &lt;- tools::file_ext(example_file)\nextension &lt;- paste0(\".\", extension)\nextension\n\n[1] \".TextGrid\"\n\n\nAs for the rest of the name, we will use the file_path_sans_ext() function that we used earlier.\n\nrest &lt;- tools::file_path_sans_ext(example_file)\nrest\n\n[1] \"shota-rep-trial_ballerinaUnacc_sgsg_related_dltc\"\n\n\n\n4.1.2 Get the subject id\nNow, the most important part is getting the subject name. If you look at what my rest variable returned, you can see that it consists of the last 4 characters, which are also the last set of characters after the last underscore. There are multiple ways to extract the subject id. I will show you both methods so that you can choose and adapt them for your own data. For the underscore version, we will use the function str_split(), and for the character counting, we will use str_sub().\n\n4.1.2.1 Underscore approach\nstr_split() takes a string and splits it according to the separator you provide. In our case, the separator is the underscore. We are also using an additional argument called simplify to make the resulting elements more user-friendly. Our function now returns a small table with 1 row and 5 columns. To select the values in the 5th column, we use square brackets again, this time with a comma. When you apply this approach to your own data, remember that you may end up with fewer or more than 5 columns depending on your naming convention. Be sure to adjust the column number accordingly. It might also be the case that your subject id is not stored last or that your separators are not underscores but simple “-”. Modify the code according to your specific needs.\n\n# Using the underscore information\nsubj &lt;- str_split(rest, \"_\", simplify = TRUE)\nsubj\n\n     [,1]              [,2]             [,3]   [,4]      [,5]  \n[1,] \"shota-rep-trial\" \"ballerinaUnacc\" \"sgsg\" \"related\" \"dltc\"\n\nsubj &lt;- subj[,5]\nsubj\n\n[1] \"dltc\"\n\n\nLastly, we have to modify the rest variable so that we do not include the subject id twice. I will use the same approach again. After obtaining the table, I will use the paste() function to concatenate the columns back together with the underscore separator. Adjust the number of columns used in this function and the separator according to your own data needs.\n\nnosubj &lt;- str_split(rest, \"_\", simplify = TRUE)\nnosubj &lt;- paste(nosubj[,1], nosubj[,2], nosubj[,3], nosubj[,4], sep = \"_\")\nnosubj\n\n[1] \"shota-rep-trial_ballerinaUnacc_sgsg_related\"\n\n\n\n4.1.2.2 Character approach\nstr_sub() allows you to extract a substring using indices. In my case, the subject id is the last four characters. To refer to characters from the end, you can use the minus symbol -. I specify -4 in the start argument, which means I want to extract the string starting from the fourth character counting back from the end.\n\nsubj &lt;- str_sub(rest, start = -4)\nsubj\n\n[1] \"dltc\"\n\n\nTo get the rest of the filename, I specify the starting point as 1 and the endpoint as -6. Using -5 would include the underscore as well.\n\nnosubj &lt;- str_sub(rest, start = 1, end = -6)\nnosubj\n\n[1] \"shota-rep-trial_ballerinaUnacc_sgsg_related\"\n\n\n\n4.1.3 Put the new name and the path together\nAt this point, we have everything we need: (i) the subject id prefix, (ii) the rest of our file name, and (iii) the extension. Now, we need to combine all of this together. We are going to use the paste0() function. Remember, this function is different from paste(). The main difference is that with paste0(), we cannot specify separators; we have to provide everything. This might seem like a disadvantage at first, but it is beneficial for non-pattern cases like this.\n\nnew_name &lt;- paste0(subj, \"_\", nosubj, extension)\nnew_name\n\n[1] \"dltc_shota-rep-trial_ballerinaUnacc_sgsg_related.TextGrid\"\n\n\nWe also need to create a new path to rename our file.\n\nnew_path &lt;- file.path(gold_dir, new_name)\nnew_path\n\n[1] \"~/data/gold/dltc_shota-rep-trial_ballerinaUnacc_sgsg_related.TextGrid\"\n\n\n\n4.1.4 Rename the file\nWe will once again use the file.rename() function. This time, we are only changing the file name and not the path, so the file will remain in its current location. We also need to obtain the full path of our example_file. We can achieve this easily using the file.path function again.\n\nexample_file_path &lt;- file.path(gold_dir, example_file)\nexample_file_path\n\n[1] \"~/data/gold/shota-rep-trial_ballerinaUnacc_sgsg_related_dltc.TextGrid\"\n\n\n\nfile.rename(example_file_path, new_path)\n\nAfter running this, make sure the naming convention is as we want. Check your folder by searching for the trial. It should look something like subj_rest.wav or subj_rest.TextGrid. In my case, it is dltc_shota-rep-trial_ballerinaUnacc_sgsg_related.TextGrid, where dltc is my subject id or subj."
  },
  {
    "objectID": "posts/mfa/index.html#little-treat-for-you",
    "href": "posts/mfa/index.html#little-treat-for-you",
    "title": "Guidelines to Run Montreal Forced Aligner for a PCIbex Experiment",
    "section": "\n4.2 Little treat for you",
    "text": "4.2 Little treat for you\nI know that some of your files look like the following: squid_S_jtfr.wav. Here, I will provide you with the code to rename this. Please check this code before using it. First, let’s arbitrarily assign this name to a variable. Remember, in your case, you will obtain this from your files list.\n\nexample_file &lt;- \"squid_S_jtfr.wav\"\n\nNow, I am going to put all the code together in one chunk, except for moving. Also, be aware that I am using my own gold_dir; please specify yours according to your needs. Additionally, be mindful of your operating system (Windows or Mac). If you are using Windows, your gold_dir variable should look like the second line. I have commented out that part with a hashtag/pound symbol. Uncomment it by deleting the first pound symbol.\n\ngold_dir &lt;- \"~/data/gold\"\n# gold_dir &lt;- \"C:/Users/utkuturk/data/gold\" # for windows\nextension &lt;- tools::file_ext(example_file)\nextension &lt;- paste0(\".\", extension)\nrest &lt;- tools::file_path_sans_ext(example_file)\nsubj &lt;- str_sub(rest, start = -4)\nnosubj &lt;- str_sub(rest, start = 1, end = -6)\nnew_name &lt;- paste0(subj, \"_\", nosubj, extension)\nnew_path &lt;- file.path(gold_dir, new_name)\nnew_path\n\n[1] \"~/data/gold/jtfr_squid_S.wav\"\n\n\nThis would be your original example file path.\n\nexample_file_path &lt;- file.path(gold_dir, example_file)\nexample_file_path\n\n[1] \"~/data/gold/squid_S_jtfr.wav\"\n\n\nAnd this line would handle the renaming from the old example_file_path to the new_path, thereby assigning the new name.\n\nfile.rename(example_file_path, new_path)"
  },
  {
    "objectID": "posts/mfa/index.html#for-loop",
    "href": "posts/mfa/index.html#for-loop",
    "title": "Guidelines to Run Montreal Forced Aligner for a PCIbex Experiment",
    "section": "\n4.3 For loop",
    "text": "4.3 For loop\nIf you have ensured that the code above works correctly for you, you are now ready to implement the for loop. Within the loop, define a variable like f and use it instead of example_file. This way, you will iterate over every file in your list. To verify that it is functioning correctly, I also added a line to print a message each time a file is renamed.\n\ngold_dir &lt;- \"~/data/gold\"\n# gold_dir &lt;- \"C:/Users/utkuturk/data/gold\" # for windows\nfiles &lt;- list.files(gold_dir, pattern = \"\\\\.wav$|\\\\.TextGrid$\")\n\nfor (f in files) {\n  extension &lt;- tools::file_ext(f)\n  extension &lt;- paste0(\".\", extension)\n  rest &lt;- tools::file_path_sans_ext(f)\n  subj &lt;- str_sub(rest, start = -4)\n  nosubj &lt;- str_sub(rest, start = 1, end = -6)\n  new_name &lt;- paste0(subj, \"_\", nosubj, extension)\n  new_path &lt;- file.path(gold_dir, new_name)\n  file_path &lt;- file.path(gold_dir, f)\n  file.rename(file_path, new_path)\n  cat(\"Renamed\", f, \"to\", new_name, \"\\n\")\n}"
  },
  {
    "objectID": "posts/mfa/index.html#operators-used-in-this-section-2",
    "href": "posts/mfa/index.html#operators-used-in-this-section-2",
    "title": "Guidelines to Run Montreal Forced Aligner for a PCIbex Experiment",
    "section": "\n4.4 Operators used in this section",
    "text": "4.4 Operators used in this section\n\ndf[selected_rows, indices_columns] or list[selected_element]\n[], Indexing operator: Accesses specific rows and/or columns of a data frame. If it is a list, it only takes a single argument to select an element. Remember in R indices start with 1, unlike python.\n\n\nselected_rows A vector of indices or names.\n\nselected_columns A vector of indices or names.\n\nselected_element A vector of indices or names.\n\nexample use: files[1]\n\n\n\n::\nDouble colon operator: Accesses functions and other objects from packages. Read x::y as ‘function y from package x.’\nexample use: tools::file_ext()"
  },
  {
    "objectID": "posts/mfa/index.html#our-task-list-2",
    "href": "posts/mfa/index.html#our-task-list-2",
    "title": "Guidelines to Run Montreal Forced Aligner for a PCIbex Experiment",
    "section": "\n4.5 Our Task List",
    "text": "4.5 Our Task List\n\nLoad the PCIbex results\nFilter irrelevant sound files\nMove all of our .wav and .TextGrid files to the same directory\nRename our files according to MFA guidelines\nRun MFA\nCreate a dataset"
  },
  {
    "objectID": "posts/mfa/index.html#moving-the-files-to-mfa-directory",
    "href": "posts/mfa/index.html#moving-the-files-to-mfa-directory",
    "title": "Guidelines to Run Montreal Forced Aligner for a PCIbex Experiment",
    "section": "\n5.1 Moving the Files to MFA Directory",
    "text": "5.1 Moving the Files to MFA Directory\n\n5.1.1 Without Dividing\nAgain, we are going to use the file.rename() and dir.create() functions to create the directory we are moving files to, and of course, to move files.\n\n# gold directory, where all of our files are\ngold_dir &lt;- \"~/data/gold\"\n# MFA directory\nmfa_dir &lt;- \"~/Documents/MFA/mycorpus\"\ndir.create(mfa_dir)\n\n# Files \nfiles &lt;- list.files(gold_dir, pattern = \"\\\\.wav$|\\\\.TextGrid$\")\nfor (f in files) {\n  old_file_path &lt;- file.path(gold_dir, f)\n  mfa_path &lt;- file.path(mfa_dir, f)\n  file.rename(old_file_path, mfa_path)\n  cat(\"Moved\", f, \"to\", mfa_dir, \"\\n\")\n}\n\n\n5.1.2 With Dividing them into subfolders\nI will introduce the following function that I use. Here, I will not go into details, but it basically performs the following steps:\n\nCreates a subfolder called s1 and moves files into it.\nCounts up to 2000.\nWhen it surpasses 2000, it creates another subfolder by incrementing the number from s1 to s2.\nContinues this process until there are no more files.\n\n\ndivide_and_move &lt;- function(source, target, limit=2000) {\n  files &lt;- list.files(source, pattern = \"\\\\.wav$|\\\\.TextGrid$\", full.names = TRUE)\n  base_names &lt;- unique(tools::file_path_sans_ext(basename(files)))\n  s_index &lt;- 1\n  f_index &lt;- 0\n  s_path &lt;- file.path(target, paste0(\"s\", s_index))\n  dir.create(s_path)\n  \n  for (b in base_names) {\n    rel_files &lt;- files[grepl(paste0(\"^\", b, \"\\\\.\"), basename(files))]\n    \n    if (f_index + length(rel_files) &gt; limit) {\n      s_index &lt;- s_index + 1\n      s_path &lt;- file.path(target, paste0(\"s\", s_index))\n      dir.create(s_path)\n      f_index &lt;- 0\n    }\n    \n    for (f in rel_files) {\n      file.rename(f, file.path(s_path, basename(f)))\n    }\n    f_index &lt;- f_index + length(rel_files)\n  }\n}\n\nYou can use this function by simply providing the source and target folders.\n\n# gold directory, where all of our files are\ngold_dir &lt;- \"~/data/gold\"\n# MFA directory\nmfa_main_dir &lt;- \"~/Documents/MFA\"\ndir.create(mfa_dir)\ndivide_and_move(gold_dir, mfa_main_dir)"
  },
  {
    "objectID": "posts/mfa/index.html#terminal-codes",
    "href": "posts/mfa/index.html#terminal-codes",
    "title": "Guidelines to Run Montreal Forced Aligner for a PCIbex Experiment",
    "section": "\n5.2 Terminal Codes",
    "text": "5.2 Terminal Codes\nAfter moving the files either with code or by hand to a specific MFA folder, ~/Documents/MFA, we can start running the terminal commands. At this point, I assume you have gone through the MFA documentation for installation instructions. I am also assuming that you have used a conda environment. If you haven’t, here are the 3 lines to install MFA.\n\n\n\nConda Installation in Terminal\n\nconda activate base\nconda install -c conda-forge mamba\nmamba create -n aligner -c conda-forge montreal-forced-aligner\n\n\nThere are again two ways to do this. One way is to open your Terminal app or use the Terminal tab in the R console below. The other way, which I prefer more, is to execute commands using the R function system(). I will first go over the easier one, which is using the Terminal app or the Terminal tab in R. But the reason I prefer the system() function is that I can loop over multiple folders more easily that way, and I do not have to run my commands again and again.\n\n5.2.1 Using Terminal\nThe first command we want to run is the conda environment code. Following the MFA documentation, I renamed my environment to aligner. So, I start by activating that environment.\n\nconda activate aligner\n\nAfter activating the environment, I need to download three models: (i) an acoustic model to recognize phonemes given previous and following acoustic features, (ii) a dictionary to access pretrained phone-word mappings, and (iii) a g2p model to generate sequences of phones based on orthography. For all of these models, we are going to use the english_us_arpa model. You can visit this website to explore various languages and models.\n\nmfa model download acoustic english_us_arpa\nmfa model download dictionary english_us_arpa\nmfa model download g2p english_us_arpa\n\nAfter downloading these models, we are going to validate our corpus. There are many customizable parameters for this step. You can check them here. I am going to use my favorite settings here. You can interpret the following command like this: Dear Montreal Forced Aligner (mfa), can you please analyze my files located in ~/Documents/MFA/mycorpus and validate them using the english_us_arpa acoustic model and english_us_arpa dictionary? Please also consider that I have multiple speakers, indicated by the first 4 characters (-s 4). It would be great to use multiprocessing (--use_mp) for faster execution. Lastly, please clean up previous and new temporary files (--clean --final_clean).\n\nmfa validate -s 4 --use_mp --clean --final_clean ~/Documents/MFA/mycorpus english_us_arpa english_us_arpa\n\nThis process will take some time. Afterward, you will have some out of vocabulary words found in your TextGrids. You can easily create new pronunciations for them and add them to your model.\nThe mfa g2p command can take many arguments; here I am using only three. First, the path to the text file that has out of vocabulary words. This file is automatically created in your folder where your files are located. The path may vary depending on your system and folder naming, but the name of the .txt file will be the same. In my case, it is ~/Documents/MFA/mycorpus/oovs_found_english_us_arpa.txt. The second argument is the name of the g2p model. As you may recall, we downloaded it earlier, and its name is english_us_arpa. Finally, the third argument is the path to a target .txt file to store new pronunciations. I would like to store them in the same place, so I am using the following path: ~/Documents/MFA/mycorpus/g2pped_oovs.txt.\n\nmfa g2p ~/Documents/MFA/mycorpus/oovs_found_english_us_arpa.txt english_us_arpa ~/Documents/MFA/mycorpus/g2pped_oovs.txt\n\nAfter creating the pronunciations, you can add them to your model with mfa model add_words. This command takes the name of the dictionary as an argument (english_us_arpa) and the output of the mfa g2p command, which was a .txt file storing pronunciations: ~/Documents/MFA/mycorpus/g2pped_oovs.txt.\n\nmfa model add_words english_us_arpa ~/Documents/MFA/mycorpus/g2pped_oovs.txt\n\nThe last step is the alignment process. It will align (mfa align) the words and the phones inside our TextGrids stored in ~/Documents/MFA/mycorpus using our previously downloaded dictionary (english_us_arpa) and model (english_us_arpa), and store the newly aligned TextGrids in a new folder called ~/Documents/MFA/output.\n\nmfa align ~/Documents/MFA/mycorpus english_us_arpa english_us_arpa ~/Documents/MFA/output\n\n\n5.2.2 Using R\nWe can also accomplish all of this in R. One advantage of this approach is that it allows us to iterate over multiple subfolders more easily, which can be useful if we have more than 2000 files. We will use four components:\n\n\nsystem() function to execute terminal commands,\n\npaste() function to create multiline templates,\n\n%s string placeholder to create template codes,\n\nsprintf() function to format our templates.\n\n\n5.2.2.1 Introduction to sprintf() and %s\n\nBefore going further with MFA codes, let me illustrate with an example. Suppose we have a list of folder names, and we want to create a .txt file in each of these folders. We can use the system() function to perform this action. Below, I define my folder_list, then create paths for my .txt files in each folder, such as ~/data1/mydocument.txt. Afterwards, I generate a list of commands to create these files using touch, which is a command-line tool for creating files. Finally, I execute these commands using the system() function.\n\nfolder_list &lt;- c(\"~/data1\", \"~/data2\", \"~/data3\")\n\n\ntxt_list &lt;- paste(folder_list, \"mydocument.txt\", sep=\"/\")\ntxt_list\ncommand_list &lt;- paste(\"touch\", txt_list, sep=\" \")\ncommand_list\n\nfor (command in command_list) {\n  system(command)\n}\n\nTechnically, we didn’t need to use a for loop; instead, we could have concatenated all these commands with ; and run a single system command. Bash can execute multiple commands in a single line when separated by ;.\n\nconcatenated_commands &lt;- paste(command_list[1], \n                               command_list[2], \n                               command_list[3], \n                               sep=\";\") \n\nsystem(concatenated_commands)\n\nWe could achieve the same without needing a folder list by utilizing the %s placeholder and the sprintf() function.\n\ncommand_template &lt;- \"touch %s/mydocument.txt\"\nconcatenated_commands &lt;- paste(sprintf(command_template, \"~/data1\"), \n                               sprintf(command_template, \"~/data2\"),\n                               sprintf(command_template, \"~/data3\"),\n                               sep=\";\")\n\nsystem(concatenated_commands)\n\nThis approach becomes particularly useful when dealing with multiple placeholders within the same command. For instance, the command template will replace the first %s with the first argument, such as ~/data1, and the second %s with the second argument, like mydoc1, when formatted using sprintf().\n\ncommand_template &lt;- \"touch %s/%s.txt\"\nconcatenated_commands &lt;- paste(sprintf(command_template, \"~/data1\", \"mydoc1\"), \n                               sprintf(command_template, \"~/data2\", \"mydoc2\"),\n                               sprintf(command_template, \"~/data3\", \"mydoc3\"),\n                               sep=\";\")\n\nsystem(concatenated_commands)\n\n\n5.2.2.2 Running MFA in R\nNext, we’ll consolidate the previous code by concatenating it using paste() and separating commands with ;. If needed, we’ll incorporate placeholders. Each line will be assigned to a new variable, and then they’ll be combined into a single command string using paste(). Finally, we’ll execute the command string using system() with the argument intern = TRUE to capture the output into an R variable, which allows for later inspection.\n\nconda_start &lt;- \"conda activate aligner\"\nget_ac &lt;- \"mfa model download acoustic english_us_arpa\"\nget_dic &lt;- \"mfa model download dictionary english_us_arpa\"\nget_g2p &lt;- \"mfa model download g2p english_us_arpa\"\n\nmfa_init &lt;- paste(conda_start, get_ac, get_dic, get_g2p, sep = \";\")\n\nmfa_init_output &lt;- system(mfa_init, intern = TRUE)\n\nAfter initializing the model, the next step involves validation. Again, I’ll use the same approach and concatenate the commands together. However, sometimes we may have too many files and need to use subfolders. To accommodate this, I’ll use %s placeholders. The validation command has one placeholder for different subfolders. Similarly, our pronunciation creation for g2p has two placeholders, though they’ll be filled with the same value. Lastly, the add_words command will use a single placeholder. Fortunately, all these folders are the same, so we can reuse the same variable repeatedly.\n\nconda_start &lt;- \"conda activate aligner\"\n\nvalidate &lt;- \"mfa validate -s 4 --use_mp --clean --final_clean ~/Documents/MFA/%s english_us_arpa english_us_arpa\"\ng2p_words &lt;- \"mfa g2p ~/Documents/MFA/%s/oovs_found_english_us_arpa.txt english_us_arpa ~/Documents/MFA/%s/g2pped_oovs.txt\"\nadd_words &lt;- \"mfa model add_words english_us_arpa ~/Documents/MFA/%s/g2pped_oovs.txt\"\n\nmfa_val &lt;- paste(conda_start, validate, g2p_words, add_words, sep = \";\")\n\nSince this step takes longer and there’s more room for errors, I want to save all my outputs in a list. First, I need to identify which folders exist in my MFA directory. Because my divide_and_move function prefixes every subfolder with s, I’ll use ^s to filter for relevant folders.\n\noutput_val &lt;- list()\n\n# Define the base path where folders are located\nbase_path &lt;- \"~/Documents/MFA\"\nfolders &lt;- list.dirs(base_path, recursive = FALSE, full.names = FALSE)\nfolders &lt;- folders[str_detect(folders, \"^s\")]\n\nfolders\n\nNow, we can iterate over this list of folders using a for loop. First, we create a temporary script using sprintf() with four placeholders. Next, we execute the current script and save the output in a temp_output variable. Later, we assign this output to specific output_name variables for each folder using paste0() and assign() functions.\n\nfor (f in folders) {\n  cur_mfa_val &lt;- sprintf(mfa_val, f, f, f, f)\n  \n  temp_output &lt;- system(cur_mfa_val, intern = TRUE)\n  \n  output_name &lt;- paste0(\"output_val_\", f)\n  \n  assign(output_name, temp_output, envir = .GlobalEnv)\n}\n\nNow you can check the outputs by calling specific variables like output_val_s1 or output_val_s2. After this step, the only task remaining is to run the aligner. We will create a template again, iterate over folders, and assign outputs to their respective names for verification. Meanwhile, the bash code will execute in the background. This time, our placeholders will refer to different inputs and an output folder. Fortunately, we can use the same output folder for every subfolder, so instead of using two placeholders, we’ll use a single %s placeholder.\n\nconda_start &lt;- \"conda activate aligner\"\n\nalign &lt;- \"mfa align ~/Documents/MFA/%s english_us_arpa english_us_arpa ~/Documents/MFA/output\"\n\nmfa_align &lt;- paste(conda_start, align, sep = \";\")\n\nfor (f in folders) {\n  cur_mfa_align &lt;- sprintf(mfa_align, f)\n  temp_output &lt;- system(cur_mfa_align, intern=TRUE)\n  output_name &lt;- paste0(\"output_align_\", f)\n  assign(output_name, temp_output, envir = .GlobalEnv)\n}\n\nThis for loop completes the MFA alignment. There is one final task remaining: creating a dataframe for further data analysis."
  },
  {
    "objectID": "posts/mfa/index.html#our-task-list-3",
    "href": "posts/mfa/index.html#our-task-list-3",
    "title": "Guidelines to Run Montreal Forced Aligner for a PCIbex Experiment",
    "section": "\n5.3 Our Task List",
    "text": "5.3 Our Task List\n\nLoad the PCIbex results\nFilter irrelevant sound files\nMove all of our .wav and .TextGrid files to the same directory\nRename our files according to MFA guidelines\nRun MFA\nCreate a dataframe"
  },
  {
    "objectID": "posts/mfa/index.html#one-file-example-1",
    "href": "posts/mfa/index.html#one-file-example-1",
    "title": "Guidelines to Run Montreal Forced Aligner for a PCIbex Experiment",
    "section": "\n6.1 One-file example",
    "text": "6.1 One-file example\nAgain, let’s work with an example file from our list, starting with the first file [1]. First, we’ll retrieve its full file path. Then, we’ll use the read_textgrid() function to create a dataframe for this single file. I’ll print the structure of the dataframe to give you a clearer view of its contents.\n\nexample_file &lt;- file_list[1]\nfile_path &lt;- file.path(tg_dir, example_file)\nexample_df &lt;- readtextgrid::read_textgrid(file_path)\nstr(example_df)\n\ntibble [36 × 10] (S3: tbl_df/tbl/data.frame)\n $ file          : chr [1:36] \"bpyuohtj_ballerina_unacc_pl_pl.TextGrid\" \"bpyuohtj_ballerina_unacc_pl_pl.TextGrid\" \"bpyuohtj_ballerina_unacc_pl_pl.TextGrid\" \"bpyuohtj_ballerina_unacc_pl_pl.TextGrid\" ...\n $ tier_num      : num [1:36] 1 1 1 1 1 1 1 1 1 1 ...\n $ tier_name     : chr [1:36] \"words\" \"words\" \"words\" \"words\" ...\n $ tier_type     : chr [1:36] \"IntervalTier\" \"IntervalTier\" \"IntervalTier\" \"IntervalTier\" ...\n $ tier_xmin     : num [1:36] 0 0 0 0 0 0 0 0 0 0 ...\n $ tier_xmax     : num [1:36] 2.52 2.52 2.52 2.52 2.52 2.52 2.52 2.52 2.52 2.52 ...\n $ xmin          : num [1:36] 0 0.57 0.61 1.11 1.37 ...\n $ xmax          : num [1:36] 0.57 0.61 1.11 1.37 1.43 ...\n $ text          : chr [1:36] \"\" \"the\" \"ballerinas\" \"next\" ...\n $ annotation_num: int [1:36] 1 2 3 4 5 6 7 8 9 10 ...\n\n\nIn this project, which involves aligning words, we are interested in only a couple of these columns. Specifically, we focus on the file identifier (file) to determine the trial from which the data originates, the tier name (tier_name) to differentiate between word and phone tiers, the start (xmin) and end (xmax) of each interval, and finally, the text. Additionally, I am not interested in retaining the file extension in the file identifier. Therefore, we will first filter to include only annotated words, then select the important columns using select(), remove the .TextGrid extension, and concatenate the words so that we can see the full response for each trial.\n\nexample_df &lt;- example_df |&gt; \n  # Filter annotated \"words\" tier\n  filter(tier_name == \"words\" & text != \"\") |&gt; \n  # Select relevant columns\n  select(file, xmin, xmax, text, annotation_num) |&gt; \n  # Remove .TextGrid and put the response together\n  mutate(file = str_remove(file, \"\\\\.TextGrid$\"),\n         response = paste(text, collapse = \" \"))\n\nexample_df\n\n\n\n\n\n\n\n\n\n\n\n\n\nfile\nxmin\nxmax\ntext\nannotation_num\nresponse\n\n\n\nbpyuohtj_ballerina_unacc_pl_pl\n0.5695\n0.6095\nthe\n2\nthe ballerinas next to the axes are shrinking\n\n\nbpyuohtj_ballerina_unacc_pl_pl\n0.6095\n1.1095\nballerinas\n3\nthe ballerinas next to the axes are shrinking\n\n\nbpyuohtj_ballerina_unacc_pl_pl\n1.1095\n1.3695\nnext\n4\nthe ballerinas next to the axes are shrinking\n\n\nbpyuohtj_ballerina_unacc_pl_pl\n1.3695\n1.4295\nto\n5\nthe ballerinas next to the axes are shrinking\n\n\nbpyuohtj_ballerina_unacc_pl_pl\n1.4295\n1.5495\nthe\n6\nthe ballerinas next to the axes are shrinking\n\n\nbpyuohtj_ballerina_unacc_pl_pl\n1.5495\n1.8995\naxes\n7\nthe ballerinas next to the axes are shrinking\n\n\nbpyuohtj_ballerina_unacc_pl_pl\n1.8995\n1.9595\nare\n8\nthe ballerinas next to the axes are shrinking\n\n\nbpyuohtj_ballerina_unacc_pl_pl\n1.9595\n2.4895\nshrinking\n9\nthe ballerinas next to the axes are shrinking\n\n\n\n\n\nWe also need some information about the trial. Luckily, all of our information is provided in our file name. So, I am going to parse that name to create a dataframe with more information. I am using a set of function that all start with separate_wider_.\n\nThe delim version uses a deliminator to split a row of a dataframe.\nThe regex version uses regular expressions to split the data.\nFinally, the position version uses the number of characters to split the data.\n\nI am doing all of this because of how I initially coded my experiment output in my PCIbex script. You may need to change this code to process your own data.\n\nexample_df &lt;- example_df |&gt;\n  # split the `file` column into 5 different columns.\n  separate_wider_delim(file, \"_\", names = c(\"subj\", \"exp\", \"headVerb\", \"NumNum\", \"sem_type\"), cols_remove = F) |&gt;\n  # split the headVerb column from the \"U\" character\n  separate_wider_regex(headVerb, c(head = \".*\", \"U\", verb_type = \".*\")) |&gt;\n  # Add the \"U\" character back to \"Unacc\" and \"Unerg\"s\n  mutate(verb_type = paste0(\"U\", verb_type)) |&gt;\n  # split the head and distractor numbers.\n  separate_wider_position(NumNum, c(head_num = 2, dist_num = 2))\n\nexample_df"
  },
  {
    "objectID": "posts/mfa/index.html#another-treat-for-you",
    "href": "posts/mfa/index.html#another-treat-for-you",
    "title": "Guidelines to Run Montreal Forced Aligner for a PCIbex Experiment",
    "section": "\n6.2 Another Treat for you",
    "text": "6.2 Another Treat for you\nLet’s see what you will need to do. You will find your file in the MFA/output folder as well, and your file will look like jtfr_squid_S.TextGrid. Let’s arbitrarily put them here. Remember, you will have to use the file.list() function as well. You will not need to change anything in the first part where we work on the TextGrid. The necessary changes will need to be done in the parsing procedure. Instead of using the entire regex or position methods, you will just need to use the delim version of the function.\n\n# Define the directory and list files\ntg_dir &lt;- \"~/Documents/MFA/output\"\nyour_file &lt;- \"jtfr_squid_S.TextGrid\"\n\nfile_path &lt;- file.path(tg_dir, your_file)\n\n\n### LETS SAY YOU RAN read_textgrid function.\n### I commented out this part, because I do not have your data.\n### Final dataframe will be slightly different here, because I do not have the textgrid data here.\n# your_df &lt;- readtextgrid::read_textgrid(path = file_path) |&gt;\n#     filter(tier_name == \"words\" & text != \"\") |&gt;\n#     select(file, xmin, xmax, text, annotation_num) |&gt;\n#     mutate(file = str_remove(file, \"\\\\.TextGrid$\"),\n#            response = paste(text, collapse = \" \"))\n#   \n\nyour_df &lt;- your_df |&gt;\n  separate_wider_delim(file, \"_\", names = c(\"subj\", \"head\", \"condition\"), cols_remove = F) \n\nyour_df\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nsubj\nhead\ncondition\nfile\nxmin\nxmax\ntext\nannotation_num\nresponse\n\n\n\njtfr\nsquid\nS\njtfr_squid_S\n\n\nsquid\n1\nsquid jumped over the fence\n\n\njtfr\nsquid\nS\njtfr_squid_S\n\n\njumped\n2\nsquid jumped over the fence\n\n\njtfr\nsquid\nS\njtfr_squid_S\n\n\nover\n3\nsquid jumped over the fence\n\n\njtfr\nsquid\nS\njtfr_squid_S\n\n\nthe\n4\nsquid jumped over the fence\n\n\njtfr\nsquid\nS\njtfr_squid_S\n\n\nfence\n5\nsquid jumped over the fence"
  },
  {
    "objectID": "posts/mfa/index.html#for-loop-1",
    "href": "posts/mfa/index.html#for-loop-1",
    "title": "Guidelines to Run Montreal Forced Aligner for a PCIbex Experiment",
    "section": "\n6.3 For-loop",
    "text": "6.3 For-loop\nNow, we need to apply this process to all files in our output directory. To simplify this, I’ll start by creating a function for processing each file individually and then apply it to all files. The function takes a file name and its directory as inputs and returns a dataframe. Before creating each dataframe, it prints “Reading the file.” to indicate progress.\n\nprocess_textgrid &lt;- function(file, directory) {\n  cat(\"Reading\", file, \"\\n\")\n  file_path &lt;- file.path(directory, file)\n  df &lt;- readtextgrid::read_textgrid(path = file_path) |&gt;\n    filter(tier_name == \"words\" & text != \"\") |&gt;\n    select(file, xmin, xmax, text, annotation_num) |&gt;\n    mutate(file = str_remove(file, \"\\\\.TextGrid$\"),\n           response = paste(text, collapse = \" \"))\n  \n  df &lt;- df |&gt;\n    separate_wider_delim(file, \"_\", names = c(\"subj\", \"exp\", \"headVerb\", \"NumNum\", \"sem_type\"), cols_remove = F) |&gt;\n    separate_wider_regex(headVerb, c(head = \".*\", \"U\", verb_type = \".*\")) |&gt;\n    mutate(verb_type = paste0(\"U\", verb_type)) |&gt;\n    separate_wider_position(NumNum, c(head_num = 2, dist_num = 2))\n  \n  return(df)\n}\n\nThis is essentially the same process as before, but encapsulated within a function for easier application. Here’s an example of how I’m redefining my directory and file list:\n\n# Define the directory and list files\ntg_dir &lt;- \"~/Documents/MFA/output\"\nfile_list &lt;- list.files(path = tg_dir, pattern = \"\\\\.TextGrid$\")\nexample_file &lt;- file_list[1]\nprocess_textgrid(example_file, tg_dir)\n\nYour version will look like this.\n\nprocess_textgrid &lt;- function(file, directory) {\n  cat(\"Reading\", file, \"\\n\")\n  file_path &lt;- file.path(directory, file)\n  df &lt;- readtextgrid::read_textgrid(path = file_path) |&gt;\n    filter(tier_name == \"words\" & text != \"\") |&gt;\n    select(file, xmin, xmax, text, annotation_num) |&gt;\n    mutate(file = str_remove(file, \"\\\\.TextGrid$\"),\n           response = paste(text, collapse = \" \"))\n  \n  df &lt;- df |&gt;\n    separate_wider_delim(file, \"_\", names = c(\"subj\", \"head\", \"condition\"), cols_remove = F) \n  \n  return(df)\n}\n\n\n# Define the directory and list files\ntg_dir &lt;- \"~/Documents/MFA/output\"\nfile_list &lt;- list.files(path = tg_dir, pattern = \"\\\\.TextGrid$\")\nexample_file &lt;- file_list[1]\nprocess_textgrid(example_file, tg_dir)\n\nNow, we need to integrate this function into our for-loop. Instead of using a single file like file_list[1], we will apply it to an entire directory. Unlike previous for loops, we will use the map function from the purrr package. It is faster and easier to use in cases like this. map() will return all of our dataframes embedded in a list. After using map(), we need to combine all these smaller dataframes into a larger one using bind_rows.\n\n# Define the directory and list files\ntg_dir &lt;- \"~/Documents/MFA/output\"\nfile_list &lt;- list.files(path = tg_dir, pattern = \"\\\\.TextGrid$\")\n# Run our function, I am using mine, you should use your own.\nprocess_textgrid &lt;- function(file, directory) {\n  cat(\"Reading\", file, \"\\n\")\n  file_path &lt;- file.path(directory, file)\n  df &lt;- readtextgrid::read_textgrid(path = file_path) |&gt;\n    filter(tier_name == \"words\" & text != \"\") |&gt;\n    select(file, xmin, xmax, text, annotation_num) |&gt;\n    mutate(file = str_remove(file, \"\\\\.TextGrid$\"),\n           response = paste(text, collapse = \" \"))\n  \n  df &lt;- df |&gt;\n    separate_wider_delim(file, \"_\", names = c(\"subj\", \"exp\", \"headVerb\", \"NumNum\", \"sem_type\"), cols_remove = F) |&gt;\n    separate_wider_regex(headVerb, c(head = \".*\", \"U\", verb_type = \".*\")) |&gt;\n    mutate(verb_type = paste0(\"U\", verb_type)) |&gt;\n    separate_wider_position(NumNum, c(head_num = 2, dist_num = 2))\n  \n  return(df)\n}\n\ndfs &lt;- map(file_list, process_textgrid, directory = tg_dir)\n\nfinal_df &lt;- bind_rows(dfs)\n\nThis completes our MFA Aligning work. We have successfully completed every task on our list, aligned our data, and created a dataframe for analysis. You can check the structure of our final dataframe, the number of rows, and the count of unique trials.\n\nstr(final_df)\n\nnrow(final_df)\n\nlength(unique(final_df$file))"
  },
  {
    "objectID": "posts/mfa/index.html#footnotes",
    "href": "posts/mfa/index.html#footnotes",
    "title": "Guidelines to Run Montreal Forced Aligner for a PCIbex Experiment",
    "section": "Footnotes",
    "text": "Footnotes\n\nOn principle, I am against for loops in R, but it is better to use here instead of confusing you more.↩︎"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Digital Castle",
    "section": "",
    "text": "My 2024 Solutions\n\n\n\n\n\n\n\n\n\n\n\n2 min\n\n\n\n\n\n\n\n\n\n\n\n\nGuidelines to Run Montreal Forced Aligner for a PCIbex Experiment\n\n\n\n\n\n\n\n\n\n\n\n42 min\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "posts/advent/2024.html#part-1",
    "href": "posts/advent/2024.html#part-1",
    "title": "My 2024 Solutions",
    "section": "Part 1",
    "text": "Part 1\n\ndf &lt;- read.table(\"2024/01/input\", sep = \"\")\nv1 &lt;- df$V1 |&gt; sort()\nv2 &lt;- df$V2 |&gt; sort()\ndiff &lt;- abs(v1-v2)\nsum(diff)\n\n[1] 2344935"
  },
  {
    "objectID": "posts/advent/2024.html#part-2",
    "href": "posts/advent/2024.html#part-2",
    "title": "My 2024 Solutions",
    "section": "Part 2",
    "text": "Part 2\n\ndf &lt;- read.table(\"2024/01/input\", sep = \"\")\n\nsimilarity_score &lt;- sum(sapply(df$V1, function(x) x * sum(df$V2 == x)))"
  },
  {
    "objectID": "posts/advent/2024.html#part-1-1",
    "href": "posts/advent/2024.html#part-1-1",
    "title": "My 2024 Solutions",
    "section": "Part 1",
    "text": "Part 1\n\nread_data &lt;- function(path) {\n  readLines(path) |&gt;\n    strsplit(\" \") |&gt;\n    lapply(as.numeric) |&gt;\n    {\n      \\(rows) {\n        max_length &lt;- max(sapply(rows, length))\n        lapply(rows, \\(x) c(x, rep(NA, max_length - length(x))))\n      }\n    }() |&gt;\n    do.call(what = rbind) |&gt;\n    as.data.frame()\n}\n\n\nsafe &lt;- function(row) { # to be used with apply functions/iterate over rows\n  row &lt;- row[!is.na(row)]\n  increasing &lt;- all(diff(row) &gt; 0)\n  decreasing &lt;- all(diff(row) &lt; 0)\n  difference &lt;- all(abs(diff(row)) &gt;= 1 & abs(diff(row)) &lt;= 3)\n  (increasing || decreasing) && difference\n}\n\ndf &lt;- read_data(\"2024/02/input\")\ndf$safe &lt;- apply(df, 1, safe)\nsum(df$safe)\n\n[1] 624"
  },
  {
    "objectID": "posts/advent/2024.html#part-2-1",
    "href": "posts/advent/2024.html#part-2-1",
    "title": "My 2024 Solutions",
    "section": "Part 2",
    "text": "Part 2\n\nread_data &lt;- function(path) {\n  readLines(path) |&gt;\n    strsplit(\" \") |&gt;\n    lapply(as.numeric) |&gt;\n    {\n      \\(rows) {\n        max_length &lt;- max(sapply(rows, length))\n        lapply(rows, \\(x) c(x, rep(NA, max_length - length(x))))\n      }\n    }() |&gt;\n    do.call(what = rbind) |&gt;\n    as.data.frame()\n}\n\nsafe &lt;- function(row) { # to be used with apply functions/iterate over rows\n  row &lt;- row[!is.na(row)]\n  increasing &lt;- all(diff(row) &gt; 0)\n  decreasing &lt;- all(diff(row) &lt; 0)\n  difference &lt;- all(abs(diff(row)) &gt;= 1 & abs(diff(row)) &lt;= 3)\n  (increasing || decreasing) && difference\n}\n\n\nprocess_row &lt;- function(row) {\n  original_row &lt;- row\n  indices_to_check &lt;- seq_along(row)\n  tried_indices &lt;- integer(0)\n\n  if (safe(row)) {return(TRUE)}\n\n  while (length(indices_to_check) &gt; 0) {\n    remove_index &lt;- sample(indices_to_check, 1)\n\n    row_with_na &lt;- row\n    row_with_na[remove_index] &lt;- NA\n\n    if (safe(row_with_na)) { return(TRUE) }\n\n    tried_indices &lt;- c(tried_indices, remove_index)\n    indices_to_check &lt;- setdiff(seq_along(row), tried_indices)\n  }\n  return(FALSE)\n}\n\ncheck_safety &lt;- function(df) {\n  df$safe &lt;- apply(df, 1, process_row)\n  return(df)\n}\n\ndf &lt;- read_data(\"2024/02/input\")\n# df$safe &lt;- apply(df, 1, safe)\ndf &lt;- check_safety(df)\nsum(df$safe)\n\n[1] 658"
  }
]